import json
import logging
import re
import uuid
from typing import Dict, Any, Awaitable, Callable # Added Awaitable, Callable
import asyncio

# Ensure aiohttp is installed: pip install aiohttp
try:
    import aiohttp
except ImportError:
    aiohttp = None # Will be checked before use

logger = logging.getLogger(__name__)

# Gemini API Configuration (placeholders, assuming Canvas provides actual key)
GEMINI_API_KEY = "AIzaSyAfjA1p4bqRIh8oUKixRri6OUfZrqi5Irg" 
GEMINI_API_URL_BASE = "https://generativelanguage.googleapis.com/v1beta/models/"
# Default model, can be overridden by specific generator functions
DEFAULT_MODEL_NAME = "gemini-2.0-flash" 

async def call_gemini_api(
    prompt: str, 
    model_name: str = DEFAULT_MODEL_NAME,
    temperature: float = 0.3, # Default temperature
    max_output_tokens: int = 8192 # Default max tokens
) -> str:
    """
    Makes an asynchronous call to the Gemini API to generate content.

    Args:
        prompt: The prompt string for the LLM.
        model_name: The specific Gemini model to use.
        temperature: Controls randomness (0.0-1.0). Lower is more deterministic.
        max_output_tokens: Maximum number of tokens to generate.

    Returns:
        The text generated by the LLM, or an error string.
    """
    if aiohttp is None:
        critical_error_msg = "Critical: aiohttp library is not installed. Please install it: pip install aiohttp"
        logger.critical(critical_error_msg)
        return f"Error: {critical_error_msg}"

    api_url = f"{GEMINI_API_URL_BASE}{model_name}:generateContent?key={GEMINI_API_KEY}"
    
    payload = {
        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": temperature,
            "topK": 1, 
            "topP": 0.95, 
            "maxOutputTokens": max_output_tokens,
            "stopSequences": [] 
        }
    }

    logger.info(f"Calling Gemini API ({model_name}) at {GEMINI_API_URL_BASE}{model_name}... Prompt length: {len(prompt)} chars.")
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(api_url, json=payload, headers={'Content-Type': 'application/json'}, timeout=300) as response: # 5 min timeout
                response_text = await response.text()
                
                if response.status == 200:
                    try:
                        result = json.loads(response_text)
                    except json.JSONDecodeError:
                        logger.error(f"Gemini API response was not valid JSON. Status: {response.status}. Response (truncated): {response_text[:500]}...")
                        return f"Error: API response was not valid JSON. Raw response: {response_text}"

                    if (result.get("candidates") and 
                        isinstance(result["candidates"], list) and len(result["candidates"]) > 0 and
                        result["candidates"][0].get("content") and
                        result["candidates"][0]["content"].get("parts") and
                        isinstance(result["candidates"][0]["content"]["parts"], list) and len(result["candidates"][0]["content"]["parts"]) > 0 and
                        result["candidates"][0]["content"]["parts"][0].get("text")):
                        
                        generated_text = result["candidates"][0]["content"]["parts"][0]["text"]
                        logger.info(f"Gemini API call successful. Received {len(generated_text)} characters.")
                        return generated_text
                    else:
                        logger.error(f"Unexpected Gemini API response structure: {json.dumps(result, indent=2)}")
                        return f"Error: Unexpected API response structure. Full response: {json.dumps(result)}"
                else:
                    logger.error(f"Gemini API call failed with status {response.status}: {response_text[:1000]}...")
                    return f"Error: API call failed with status {response.status}. Details: {response_text}"
        except aiohttp.ClientConnectorError as e:
            logger.error(f"Network connection error: Could not connect to Gemini API at {api_url}. Error: {e}")
            return f"Error: Could not connect to API. Details: {e}"
        except asyncio.TimeoutError:
            logger.error(f"Gemini API call timed out after 300 seconds for URL: {api_url}")
            return "Error: API call timed out."
        except Exception as e:
            logger.error(f"An unexpected error occurred during Gemini API call: {e}", exc_info=True)
            return f"Error: An unexpected error occurred during API call. Details: {e}"

def parse_llm_output_to_files(llm_response: str) -> Dict[str, str]:
    """
    Parses the LLM's multi-file output (using '=== FILE: ... ===' markers)
    into a dictionary of {filepath: content}.
    This version attempts to strip common markdown code fences.
    """
    generated_files: Dict[str, str] = {}
    current_filepath: str | None = None
    current_content_lines: list[str] = []

    # Regex to capture filepath from marker, allowing for optional spaces and backticks
    # e.g., === FILE: path/to/file.py ===
    # or    === FILE: `path/to/file.py` ===
    file_marker_regex = re.compile(r"^=== FILE:\s*`?(.*?)`?\s*===$")

    # Regex to detect common markdown code fences with optional language specifier
    # e.g., ```python, ```java, ```
    code_fence_start_regex = re.compile(r"^\s*```(\w*\s*)$") # Captures optional language
    code_fence_end_regex = re.compile(r"^\s*```\s*$")

    if not llm_response:
        logger.warning("LLM response is empty, cannot parse files.")
        generated_files[f"llm_empty_response_{uuid.uuid4().hex[:4]}.txt"] = llm_response
        return generated_files
    
    if llm_response.startswith("Error:"):
        logger.warning(f"LLM response is an error message. Storing as error file. Response: {llm_response[:200]}")
        generated_files[f"llm_error_response_passed_{uuid.uuid4().hex[:4]}.txt"] = llm_response
        return generated_files

    lines = llm_response.splitlines()
    line_iterator = iter(lines)

    for line in line_iterator:
        marker_match = file_marker_regex.match(line)
        if marker_match:
            if current_filepath and current_content_lines:
                # Process and store the content for the previous file
                content_str = "\n".join(current_content_lines).strip()
                
                # Strip common code fences
                # Check if the first line of content is a start fence
                # and last line is an end fence
                temp_lines = content_str.splitlines()
                if temp_lines:
                    if code_fence_start_regex.match(temp_lines[0]) and \
                       (len(temp_lines) == 1 or code_fence_end_regex.match(temp_lines[-1])): # Handles single line ```code``` or multi-line
                        
                        # Remove starting fence line
                        cleaned_content = "\n".join(temp_lines[1:]) 
                        
                        # Remove ending fence line if it exists and content is not just the fence itself
                        if cleaned_content.strip().endswith("```"):
                            # Find the last occurrence of ``` and strip from there
                            last_fence_index = cleaned_content.rfind("```")
                            if last_fence_index != -1:
                                cleaned_content = cleaned_content[:last_fence_index]
                        
                        content_str = cleaned_content.strip()
                    elif code_fence_end_regex.match(temp_lines[0]) and len(temp_lines) == 1: # Edge case: only ``` on a line after marker
                        content_str = "" # No actual content
                        
                generated_files[current_filepath] = content_str

            # Extract new filepath
            current_filepath = marker_match.group(1).strip()
            if not current_filepath:
                logger.warning(f"Parsed an empty filepath from marker: {line}. Using fallback name.")
                current_filepath = f"unknown_file_{len(generated_files)}_{uuid.uuid4().hex[:4]}.txt"
            current_content_lines = []
        elif current_filepath is not None:
            current_content_lines.append(line)

    # Add the last file's content after the loop finishes
    if current_filepath and current_content_lines:
        content_str = "\n".join(current_content_lines).strip()
        temp_lines = content_str.splitlines()
        if temp_lines:
            if code_fence_start_regex.match(temp_lines[0]) and \
               (len(temp_lines) == 1 or code_fence_end_regex.match(temp_lines[-1])):
                cleaned_content = "\n".join(temp_lines[1:])
                if cleaned_content.strip().endswith("```"):
                    last_fence_index = cleaned_content.rfind("```")
                    if last_fence_index != -1:
                        cleaned_content = cleaned_content[:last_fence_index]
                content_str = cleaned_content.strip()
            elif code_fence_end_regex.match(temp_lines[0]) and len(temp_lines) == 1:
                content_str = ""
        generated_files[current_filepath] = content_str
    
    if not generated_files:
        logger.warning("LLM output parsing did not yield any structured files, though response was not an error string. Saving raw response.")
        generated_files[f"llm_raw_unparsed_response_{uuid.uuid4().hex[:4]}.txt"] = llm_response
        
    return generated_files

if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)
    
    async def test_api_call_example():
        test_prompt = "Briefly explain the concept of a Large Language Model in one paragraph."
        response = await call_gemini_api(test_prompt, temperature=0.7)
        print("--- Gemini API Test Response ---")
        print(response)

    def test_parser_example():
        mock_llm_output = """
=== FILE: main.py ===
print("Hello from main.py")

a = 1 + 1
=== FILE: utils/helper.py ===
def greet(name):
    return f"Hello, {name}"
=== FILE: requirements.txt ===
fastapi
uvicorn
        """
        files = parse_llm_output_to_files(mock_llm_output)
        print("\n--- Parsed Files Test ---")
        for path, content in files.items():
            print(f"File: {path}\nContent:\n{content}\n---")

        error_output = "Error: API call failed."
        files_from_error = parse_llm_output_to_files(error_output)
        print("\n--- Parsed Files from Error String ---")
        for path, content in files_from_error.items():
            print(f"File: {path}\nContent:\n{content}\n---")
        
        empty_output = ""
        files_from_empty = parse_llm_output_to_files(empty_output)
        print("\n--- Parsed Files from Empty String ---")
        for path, content in files_from_empty.items():
            print(f"File: {path}\nContent:\n{content}\n---")

    # To run the API test (requires aiohttp and network):
    # asyncio.run(test_api_call_example())
    
    # To run the parser test:
    test_parser_example()
